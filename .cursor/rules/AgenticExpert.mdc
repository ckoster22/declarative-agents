---
description: 
globs: *agent.py,*orchestrator.py,*agents.py
alwaysApply: false
---
# Agentic Expert: OpenAI Agent Development Best Practices

## 1. Agent Architecture & Design

### Core Components
- **LLM Core:** The central language model (e.g., GPT-4, GPT-3.5-turbo) that processes information and generates responses.
- **Tools/Functions:** External capabilities the agent can use to interact with its environment, fetch data, or perform actions.
- **Memory:** Mechanisms for storing and retrieving information from previous interactions or knowledge bases. Short-term for conversation history, long-term for persistent knowledge.
- **Planning Module:** (Optional but recommended for complex tasks) A component that breaks down complex goals into smaller, manageable steps.
- **Execution Engine:** Orchestrates the flow, calling the LLM, tools, and memory as needed based on the plan.

### Design Principles
- **Modularity:** Design components (LLM interaction, tool use, memory, planning) as loosely coupled modules.
- **Clarity of Purpose:** Each agent should have a well-defined objective or set of tasks it's designed to perform.
- **Iterative Development:** Start simple and progressively add complexity and capabilities.
- **User-Centricity:** Design with the end-user's needs and interaction style in mind.
- **Observability:** Implement logging and monitoring to understand agent behavior and debug issues.
- **Safety and Responsibility:** Design agents to be helpful, honest, and harmless. Implement safeguards against misuse.

### State Management
- **Context Window Awareness:** Be mindful of the LLM's context window limitations. Summarize or prune context as needed.
- **Session Management:** Clearly define how an agent's state is maintained across multiple turns in a conversation.
- **Error States:** Design how the agent handles and recovers from errors (e.g., tool failures, LLM refusals).

## 2. Tool Development & Integration

### Tool Design Patterns
- **Specificity:** Tools should perform well-defined, specific actions.
- **Atomicity:** Design tools to be atomic where possible, or provide clear transactional semantics.
- **Clear Descriptions:** Provide concise, accurate, and unambiguous descriptions for tools/functions so the LLM can choose and use them effectively. Include parameter descriptions and expected output formats.
- **Robustness:** Tools should handle a variety of inputs gracefully and return informative errors.
- **Idempotency:** Where applicable, design tools to be idempotent (multiple calls with the same input have the same effect as one call).

### Error Handling & Validation
- **Structured Errors:** Tools should return structured error messages that the agent (or LLM) can understand and potentially act upon.
- **Input Validation:** Validate inputs to tools before execution to prevent unexpected behavior.
- **Retry Mechanisms:** Implement retry logic (e.g., exponential backoff) for transient tool failures, especially for API calls.

### Tool Registration & Management
- **Dynamic Loading:** Consider mechanisms for dynamically loading or updating tools if the agent's capabilities need to evolve.
- **Versioning:** Version your tools to manage changes and ensure backward compatibility.

### Security Considerations for Tools
- **Least Privilege:** Tools should only have the permissions necessary to perform their intended function.
- **Input Sanitization:** Sanitize any inputs passed to tools, especially if they execute code or interact with external systems.
- **Output Scrutiny:** Scrutinize tool outputs before presenting them to the user or feeding them back into the LLM.
- **Authentication/Authorization:** Secure tool endpoints appropriately.

## 3. Memory & Context Management

### Short-Term Memory
- **Conversation History:** Maintain a rolling window of the recent conversation.
- **Summarization:** For long conversations, use the LLM to summarize past turns to save context space.
- **Key Information Extraction:** Identify and store key entities or pieces of information from the conversation.

### Long-Term Memory
- **Vector Databases:** For semantic search over large knowledge bases (e.g., RAG - Retrieval Augmented Generation).
- **Knowledge Graphs:** For storing and querying structured relationships.
- **User Profiles:** Store user preferences or past interactions to personalize responses.
- **Scratchpad:** A temporary area for the agent to "think" or store intermediate results.

### Context Window Optimization
- **Pruning:** Remove less relevant parts of the conversation history.
- **Summarization:** Condense information to fit within the context limit.
- **Selective Retrieval:** Only load relevant information from long-term memory into the context.

## 4. Planning & Decision Making

### Task Decomposition
- **Chain-of-Thought (CoT):** Encourage the LLM to "think step by step" to break down problems.
- **Tree of Thoughts (ToT):** Explore multiple reasoning paths and self-critique.
- **Goal-Oriented Planning:** Define high-level goals and allow the agent to generate sub-goals and plans.
- **Hierarchical Planning:** Break down tasks into a hierarchy of abstraction levels.

### Planning Patterns
- **ReAct (Reason + Act):** Combine reasoning traces with action execution. The LLM generates thoughts about what to do, then chooses an action (tool), observes the result, and repeats.
- **Self-Correction/Critique:** Prompt the agent to review its plan or output and identify areas for improvement.
- **Tool Selection Logic:** Ensure the LLM has clear instructions or examples for when and how to use available tools.

### Handling Uncertainty & Edge Cases
- **Fallback Mechanisms:** Define behavior when a plan fails or an unexpected situation arises.
- **Clarification Prompts:** Allow the agent to ask clarifying questions if a request is ambiguous.
- **Confidence Scoring:** If possible, have the LLM or tools indicate confidence in their outputs.

## 5. Performance & Optimization

### Token Usage Optimization
- **Concise Prompts:** Write clear but brief prompts.
- **Few-Shot Learning:** Use a small number of well-chosen examples instead of lengthy instructions.
- **Summarization:** Summarize large texts or conversation histories.
- **Caching:** Cache LLM responses for identical or similar queries where appropriate.
- **Model Selection:** Use smaller, faster models for tasks that don't require the most powerful model's capabilities.

### Response Time Considerations
- **Streaming:** Stream responses from the LLM and tools to improve perceived latency.
- **Parallelization:** Execute independent tool calls or LLM queries in parallel if possible.
- **Asynchronous Operations:** Use asynchronous patterns for long-running tool calls.

### Cost Management
- **Monitor API Usage:** Track token consumption and associated costs.
- **Set Budgets/Limits:** Implement mechanisms to prevent runaway costs.
- **Optimize Tool Calls:** Avoid unnecessary tool calls.

## 6. Testing & Validation

### Testing Methodologies
- **Unit Tests:** Test individual tools and small agent components.
- **Integration Tests:** Test the interaction between the LLM, tools, and memory.
- **End-to-End (E2E) Tests:** Simulate full user interactions and evaluate overall task completion and quality.
- **Golden Datasets:** Create a set of benchmark prompts and expected outcomes to track performance over time.
- **Human Evaluation:** Regularly have humans review and rate agent responses for quality, relevance, and safety.

### Validation Strategies
- **Output Parsing & Validation:** Ensure the LLM's output (e.g., JSON for tool calls) conforms to the expected schema.
- **Hallucination Detection:** Implement strategies to detect and mitigate LLM hallucinations (e.g., by grounding responses in retrieved documents).
- **Fact-Checking:** Cross-reference LLM outputs with reliable sources if critical accuracy is needed.

### Error Recovery Patterns
- **Self-Correction Prompts:** Design prompts that allow the agent to retry or correct itself after an error.
- **Graceful Degradation:** Ensure the agent can still provide a useful (if limited) response if a component fails.

## 7. Security & Safety with OpenAI Agents SDK

Building safe and secure AI agents is paramount.

### Input Validation & Sanitization
- **Prompt Injection Defense:**
    - **Clear Delimiters & Instructions:** Use clear delimiters in prompts to separate instructions from user input. Instruct the LLM to only pay attention to user input within specific markers.
    - **`InputGuardrail`:** This is a key SDK mechanism. Implement an `InputGuardrail` function (potentially using another LLM with specific instructions) to analyze user input for malicious patterns (like attempts to overwrite original instructions) before the main `Agent` processes it. The `input_guardrails.py` example shows how an agent (`guardrail_agent`) can be used for this. `GuardrailFunctionOutput.tripwire_triggered` can stop a problematic run.
    - **Re-prompting/Escaping:** If user input is directly concatenated into complex prompts, ensure it's appropriately escaped or structured to prevent misinterpretation by the LLM.
- **Data Sanitization for Tools:** As covered in "Security Considerations for Tools" (Section 2.4), always sanitize and validate data passed as `ToolCallItem.tool_input` before tools use it, especially for operations like database queries or shell commands.

### Output Sanitization & Control
- **Prevent Harmful Content:**
    - **`OutputGuardrail`:** Similar to `InputGuardrail`, use an `OutputGuardrail` to inspect the `Agent`'s final `MessageOutputItem` before it's sent to the user. This guardrail can check for harmful, biased, or inappropriate content, and trigger a `GuardrailFunctionOutput.tripwire_triggered` to prevent sending or to modify the message. The `output_guardrails.py` example illustrates this.
    - **Prompting for Safe Responses:** Instruct agents to avoid generating certain types of content.
- **Data Leakage Prevention:**
    - **Tool Design:** Ensure tools do not inadvertently expose sensitive data in their `ToolCallOutputItem.output`.
    - **Agent Instructions:** Instruct agents not to reveal sensitive information they might have access to (e.g., via `RunContextWrapper` or tool outputs from internal systems).
    - **`OutputGuardrail`:** Can be used to scan for and redact sensitive data patterns before output.

### Rate Limiting & Abuse Prevention (Application Layer)
- **SDK Scope:** The OpenAI Agents SDK itself does not provide built-in rate limiting for agent invocations or tool usage beyond what the underlying OpenAI API might enforce.
- **Application Responsibility:** Implement rate limiting, usage quotas, and abuse monitoring at the application layer that invokes `Runner.run()`.
- **Monitoring Agent Behavior:** Use logging, `@trace`, and `AgentHooks` to monitor for unusual patterns of tool use or agent interactions that might indicate misuse.

### Secure Tool Usage
- Refer to "Security Considerations for Tools" (Section 2.4) for detailed points on least privilege, input sanitization for tools, output scrutiny from tools, and secure authentication/authorization for tool endpoints.
- Be particularly cautious with tools that can execute code or modify external state.

## 8. Documentation & Maintenance for OpenAI Agents SDK Projects

Good documentation and clear maintenance procedures are vital for the longevity and evolution of agentic systems.

### Code & Configuration Documentation
- **Docstrings:**
    - Provide clear docstrings for all `Agent` classes (if you subclass), tool functions (`@function_tool`), `GuardrailFunction`s, and `AgentHooks` implementations.
    - For tool functions, the docstring's first line is often used as the `description` in the `ToolDefinition` sent to the LLM, and parameter descriptions in the docstring are also used. Be precise.
- **Tool Definitions (`ToolDefinition`):** Ensure `name_override` and `description_override` (if used with `@function_tool`) or the fields in a manually created `ToolDefinition` are accurate and kept up-to-date with the tool's functionality. This is critical for the LLM's tool selection.
- **Agent Instructions (`Agent.instructions`):** Version control your agent instructions. Small changes here can significantly alter behavior. Keep a log of impactful prompt iterations.
- **Pydantic Models (`output_type`, Context Models):** Document the purpose of each field in your Pydantic models used for `output_type`, `RunContextWrapper` context, or tool arguments.
- **Configuration Files:** If you externalize agent configurations (e.g., model names, temperature settings in `ModelSettings`), document these configuration files.

### System Architecture Documentation
- **Agent Interaction Diagrams:** For multi-agent systems, diagram the flow of control, including `HandoffOutputItem` paths, and how `Agent`s call each other (e.g., via `routing.py` patterns or `agents_as_tools.py`).
- **Data Flow Diagrams:** Illustrate how data (user input, `TResponseInputItem` history, `RunContextWrapper` data, tool outputs) flows through the system.
- **Key SDK Components:** Document how core SDK components like `Agent`, `Runner`, `ToolDefinition`, `GuardrailFunction`, and `AgentHooks` are utilized in your specific architecture.
- **Design Decisions:** Record the rationale for choosing specific agent designs, tool granularities, context management strategies, and guardrail implementations.

### Maintenance Procedures
- **Regular Audits:** Periodically review agent performance (latency, token usage), safety (guardrail effectiveness, harmful outputs), and alignment with objectives using golden datasets and human evaluation.
- **Model & SDK Updates:**
    - Have a plan for evaluating and integrating new LLM versions (specified in `OpenAIChatCompletionsModel(model=...)`). Test thoroughly, as new models can change behavior.
    - Keep the OpenAI Agents SDK (`openai-agents`) and other dependencies up-to-date. Review SDK changelogs for breaking changes or new features.
- **Tool Maintenance:** Regularly review and test tools, especially those interacting with external APIs that might change.
- **Feedback Loop:**
    - Implement mechanisms for collecting user feedback on agent interactions.
    - Use this feedback, along with monitoring data (from `@trace`, `AgentHooks`, logs), to iteratively improve agent instructions, tools, and guardrails.

## 9. Examples from openai/openai-agents-python

### `agents_as_tools.py`
This example directly illustrates the **Modularity & Composability** principle (Section 1.2) and **Hierarchical Planning with Multiple Agents** (Section 4.1), showing how an orchestrator `Agent` can delegate tasks to specialized `Agent`s by using them as tools.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from agents import Agent, Runner

# This example shows how to use agents as tools.
# This is useful for decomposing a task into subtasks that can be handled by specialized agents.
# For example, you might have a generalist agent that can delegate to specialist agents for specific tasks.

# Create two specialist agents
spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)
french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

# Create an orchestrator agent that uses the specialist agents as tools
orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)


async def main() -> None:
    # Run the orchestrator agent
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(f"Spanish translation: {result.final_output}")

    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in French.")
    print(f"French translation: {result.final_output}")

    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish and French.")
    # Note: the final output is the text from the orchestrator agent, not the specialist agents.
    # If you want to see the specialist agent output, you can inspect result.new_items
    print(f"Orchestrator response to multi-request: {result.final_output}")


if __name__ == "__main__":
    asyncio.run(main())
```

### `deterministic.py`
This example showcases the **Deterministic Behavior (where needed)** principle (Section 1.2) by using `output_type` with Pydantic models to ensure structured, predictable outputs from agents, which is also crucial for **Output Parsing & Validation** (Section 6.2).
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from pydantic import BaseModel

from agents import Agent, Runner, TResponseInputItem

# This example shows how to use agents for deterministic workflows.
# You can pass `output_type` to an agent to specify the output format.
# This ensures the output is a valid object of that type.
# You can then use the output of one agent as the input to another.

# Define the output type for the first agent
class UserInfo(BaseModel):
    name: str
    age: int


# Create the first agent
user_info_agent = Agent(
    name="User info agent",
    instructions="Extract the user's name and age from the input.",
    output_type=UserInfo,
)


# Define the output type for the second agent
class Greeting(BaseModel):
    greeting: str


# Create the second agent
greeting_agent = Agent(
    name="Greeting agent",
    instructions="Greet the user by name and tell them their age.",
    output_type=Greeting,
)


async def main() -> None:
    # Run the first agent
    result1 = await Runner.run(user_info_agent, input="My name is John and I am 30 years old.")
    # The output is a UserInfo object
    user_info = result1.final_output_as(UserInfo)
    print(f"User info: {user_info}")

    # Run the second agent, using the output of the first agent as input
    # We convert the UserInfo object to a dict, and then pass it as a tool_call_output_item
    # so the greeting agent can use it.
    input_items: list[TResponseInputItem] = [
        {
            "type": "tool_call_output_item",
            "tool_name": "user_info_tool",  # Pretend this was a tool call
            "output": user_info.model_dump_json(),
        }
    ]
    result2 = await Runner.run(greeting_agent, input=input_items)
    # The output is a Greeting object
    greeting = result2.final_output_as(Greeting)
    print(f"Greeting: {greeting}")


if __name__ == "__main__":
    asyncio.run(main())
```

### `forcing_tool_use.py`
This example demonstrates a specific aspect of **Tool Selection Logic** (Section 4.2) and **Security Considerations for Tools** (Section 2.4), where `ModelSettings(tool_choice=...)` is used to compel an `Agent` to use a particular tool.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from agents import Agent, ModelSettings, Runner, function_tool

# This example shows how to force an agent to use a specific tool.
# This is useful for ensuring that an agent uses a tool when it is needed.
# You can do this by setting `tool_choice` in the `model_settings` of an agent.


@function_tool
def get_weather(city: str) -> str:
    """Get the weather for a city."""
    if city == "London":
        return "Rainy"
    elif city == "Paris":
        return "Sunny"
    else:
        return "Unknown"


# Create an agent that is forced to use the get_weather tool
# Note the use of `tool_choice` in `model_settings`.
# This forces the agent to use the `get_weather` tool.
# This is useful if you want to ensure that the agent always uses this tool.
agent = Agent(
    name="Weather agent",
    instructions="You are a weather agent. You must use the get_weather tool to get the weather.",
    tools=[get_weather],
    model_settings=ModelSettings(tool_choice={"type": "function", "function": {"name": "get_weather"}}),
)


async def main() -> None:
    # Run the agent
    result = await Runner.run(agent, input="What is the weather in London?")
    print(f"Weather in London: {result.final_output}")

    result = await Runner.run(agent, input="What is the weather in Paris?")
    print(f"Weather in Paris: {result.final_output}")


if __name__ == "__main__":
    asyncio.run(main())
```

### `input_guardrails.py`
This file is a practical demonstration of **Safety & Responsibility** (Section 1.2), specifically implementing an `InputGuardrail` for **Input Validation & Sanitization** (Section 7.1) and **Guardrails for Decision Making** (Section 4.3). It also shows an agent (`guardrail_agent`) being used to power another agent's guardrail logic.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from pydantic import BaseModel

from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrail,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
)

# This example shows how to use input guardrails to validate user input.
# Input guardrails are functions that are run before an agent processes the input.
# If an input guardrail triggers a tripwire, the agent run is halted and an exception is raised.
# This is useful for preventing agents from processing malicious or irrelevant input.


# Define the output type for the guardrail agent
# This is the type of the output that the guardrail agent will produce.
# It should include a boolean field that indicates whether the tripwire should be triggered.
class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str


# Create a guardrail agent
# This agent will be used by the input guardrail to check if the input is math homework.
guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


# Define an input guardrail function
# This function will be run before the main agent processes the input.
# It uses the guardrail agent to check if the input is math homework.
# If it is, it triggers a tripwire by returning `tripwire_triggered=True`.
async def math_guardrail(
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)
    math_homework_output = result.final_output_as(MathHomeworkOutput)
    return GuardrailFunctionOutput(
        output_info=math_homework_output,
        tripwire_triggered=math_homework_output.is_math_homework,
    )


# Create the main agent
# This agent will process the user input if the input guardrail does not trigger a tripwire.
# It has the math_guardrail function as an input guardrail.
main_agent = Agent(
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[InputGuardrail(guardrail_function=math_guardrail)],
)


async def main() -> None:
    # This should trip the guardrail because the input is math homework
    try:
        await Runner.run(main_agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")
    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped as expected.")

    # This should not trip the guardrail because the input is not math homework
    try:
        result = await Runner.run(main_agent, "Hello, I have a question about my account.")
        print(f"Guardrail didn't trip, agent response: {result.final_output}")
    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped - this is unexpected")


if __name__ == "__main__":
    asyncio.run(main())
```

### `llm_as_a_judge.py`
This example vividly illustrates **Self-Correction/Critique with Multi-Agent Systems** (Section 4.2) and **Multi-Agent Self-Correction (LLM as Judge)** (Section 6.3), where one `Agent` evaluates and provides feedback on another `Agent`'s output for iterative refinement.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from pydantic import BaseModel

from agents import Agent, Runner, TResponseInputItem

# This example shows how to use an LLM as a judge to evaluate the output of another LLM.
# This is useful for tasks like content moderation, quality control, and bias detection.
# It involves a "worker" agent that performs a task and a "judge" agent that evaluates the output.
# The judge agent can provide feedback to the worker agent, which can then revise its output.


# Define the worker agent
# This agent will write a short story based on a given prompt.
worker_agent = Agent(
    name="Story writer",
    instructions="You are a story writer. Write a short story based on the given prompt.",
)

# Define the output type for the judge agent
# This specifies the format of the judge's evaluation.
# It includes a rating, feedback, and a boolean indicating if the story is acceptable.
class JudgeOutput(BaseModel):
    rating: int
    feedback: str
    acceptable: bool


# Define the judge agent
# This agent will evaluate the story written by the worker agent.
# It uses the JudgeOutput type to structure its evaluation.
judge_agent = Agent(
    name="Story judge",
    instructions=(
        "You are a story judge. Evaluate the story based on creativity, coherence, and grammar. "
        "Provide a rating from 1 to 5, constructive feedback, and indicate if the story is acceptable."
    ),
    output_type=JudgeOutput,
)


async def main() -> None:
    max_revisions = 2
    current_story = ""
    prompt = "Write a story about a brave knight and a friendly dragon."
    judge_feedback = None # Initialize judge_feedback

    for i in range(max_revisions + 1):
        print(f"
--- Attempt {i + 1} ---")
        # Worker agent generates or revises the story
        worker_input: str | list[TResponseInputItem]
        if i == 0:
            worker_input = prompt
            print(f"Prompt: {prompt}")
        else:
            # Create input for revision, including previous story and judge's feedback
            # Ensure judge_feedback is not None before accessing its attributes
            feedback_content = judge_feedback.feedback if judge_feedback else "No feedback provided."
            worker_input = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": current_story},
                {
                    "role": "user",
                    "content": f"Please revise the story based on the following feedback: {feedback_content}",
                },
            ]
            print(f"Revising story with feedback: {feedback_content}")

        story_result = await Runner.run(worker_agent, input=worker_input)
        current_story = story_result.final_output_as(str)
        print(f"Story: {current_story}")

        # Judge agent evaluates the story
        judge_result = await Runner.run(judge_agent, input=current_story)
        judge_feedback = judge_result.final_output_as(JudgeOutput)
        print(f"Judge Rating: {judge_feedback.rating}")
        print(f"Judge Feedback: {judge_feedback.feedback}")
        print(f"Acceptable: {judge_feedback.acceptable}")

        if judge_feedback.acceptable:
            print("
Story accepted!")
            break
        elif i == max_revisions:
            print("
Max revisions reached. Story not accepted.")
            break


if __name__ == "__main__":
    asyncio.run(main())
```

### `output_guardrails.py`
This file demonstrates **Safety & Responsibility** (Section 1.2) by implementing an `OutputGuardrail` for **Output Sanitization & Control** (Section 7.2) and **Guardrails for Decision Making** (Section 4.3).
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from pydantic import BaseModel

from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrail,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
)

# This example shows how to use output guardrails to validate agent output.
# Output guardrails are functions that are run after an agent produces its final output.
# If an output guardrail triggers a tripwire, an exception is raised.
# This is useful for ensuring that agent output meets certain criteria,
# such as not containing sensitive information or adhering to a specific format.


# Define the output type for the main agent
# This is the type of the output that the main agent will produce.
class MessageOutput(BaseModel):
    response: str


# Define the output type for the guardrail agent
# This is the type of the output that the guardrail agent will produce.
# It should include a boolean field that indicates whether the tripwire should be triggered.
class ContainsMathOutput(BaseModel):
    is_math: bool
    reasoning: str


# Create a guardrail agent
# This agent will be used by the output guardrail to check if the output contains math.
guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=ContainsMathOutput,
)


# Define an output guardrail function
# This function will be run after the main agent produces its output.
# It uses the guardrail agent to check if the output contains math.
# If it does, it triggers a tripwire by returning `tripwire_triggered=True`.
async def math_output_guardrail(
    ctx: RunContextWrapper[None], agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)
    contains_math_output = result.final_output_as(ContainsMathOutput)
    return GuardrailFunctionOutput(
        output_info=contains_math_output,
        tripwire_triggered=contains_math_output.is_math,
    )


# Create the main agent
# This agent will produce output that is then checked by the output guardrail.
# It has the math_output_guardrail function as an output guardrail.
main_agent = Agent(
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[OutputGuardrail(guardrail_function=math_output_guardrail)],
    output_type=MessageOutput,  # The agent's output will be of this type
)


async def main() -> None:
    # This should trip the guardrail because the agent's response will contain math
    try:
        await Runner.run(main_agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")
    except OutputGuardrailTripwireTriggered as e:
        print(f"Math output guardrail tripped as expected. Details: {e.guardrail_result.output_info}")

    # This should not trip the guardrail because the agent's response will not contain math
    try:
        result = await Runner.run(main_agent, "Hello, I have a question about my account.")
        print(f"Guardrail didn't trip, agent response: {result.final_output_as(MessageOutput).response}")
    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped - this is unexpected")


if __name__ == "__main__":
    asyncio.run(main())
```

### `parallelization.py`
This example showcases **Parallelization (`asyncio.gather`)** (Section 5.2) for improving **Response Time Considerations** by running multiple independent `Runner.run` calls concurrently.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from agents import Agent, Runner

# This example demonstrates how to run multiple agent interactions in parallel.
# This is useful when you have multiple independent tasks that can be processed simultaneously,
# potentially speeding up the overall workflow.

# Define a simple agent that will be used for parallel tasks
summarizer_agent = Agent(
    name="Summarizer",
    instructions="Summarize the given text in one sentence.",
)

texts_to_summarize = [
    "The quick brown fox jumps over the lazy dog. This classic pangram contains all letters of the English alphabet.",
    "Artificial intelligence is rapidly transforming various industries, from healthcare to finance.",
    "The Eiffel Tower, located in Paris, France, was completed in 1889 and is an iconic landmark.",
    "Climate change is a pressing global issue that requires immediate and collective action.",
]


async def main() -> None:
    # Create a list of tasks (Runner.run calls) to be executed in parallel
    tasks = []
    for i, text_content in enumerate(texts_to_summarize):
        # Each Runner.run call is an awaitable, suitable for asyncio.gather
        task = Runner.run(
            starting_agent=summarizer_agent,
            input=text_content,
            # You can add run_config to name workflows if desired for tracing
            # run_config=RunConfig(workflow_name=f"Summary Task {i+1}")
        )
        tasks.append(task)

    # Use asyncio.gather to run all tasks concurrently
    print(f"Starting {len(tasks)} summarization tasks in parallel...")
    results = await asyncio.gather(*tasks)
    print("All tasks completed.")

    # Process and print the results
    for i, result in enumerate(results):
        summary = result.final_output_as(str)
        print(f"
Original Text {i+1}: {texts_to_summarize[i]}")
        print(f"Summary {i+1}: {summary}")


if __name__ == "__main__":
    asyncio.run(main())
```

### `routing.py`
This example directly illustrates **Modularity & Composability** (Section 1.2) and **Hierarchical Planning with Multiple Agents** (Section 4.1), where a `router_agent` uses `output_type` to make a decision and route tasks to appropriate specialist `Agent`s.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio

from pydantic import BaseModel, Field

from agents import Agent, Runner

# This example demonstrates routing logic based on the output of an LLM.
# A "router" agent determines which "specialist" agent should handle a query.
# This pattern is useful for directing tasks to the most appropriate agent based on query content.

# Define specialist agents for different topics
math_agent = Agent(
    name="Math Specialist",
    instructions="You are a math expert. Solve the math problem provided.",
)

history_agent = Agent(
    name="History Specialist",
    instructions="You are a history expert. Answer the historical question provided.",
)

# Define the output type for the router agent
# This specifies the structure of the router's decision.
# It includes the chosen specialist and the reasoning.
class RouteDecision(BaseModel):
    specialist_name: str = Field(description="Name of the specialist agent to route to (e.g., 'Math Specialist').")
    reasoning: str = Field(description="Brief explanation for choosing this specialist.")


# Define the router agent
# This agent decides which specialist agent to route the query to.
# It uses the RouteDecision output type.
router_agent = Agent(
    name="Query Router",
    instructions=(
        "Analyze the user's query and decide which specialist agent (Math Specialist or History Specialist) "
        "is best suited to answer it. Provide the name of the chosen specialist and your reasoning."
    ),
    output_type=RouteDecision,
)

specialist_agents = {
    "Math Specialist": math_agent,
    "History Specialist": history_agent,
}


async def process_query(query: str) -> None:
    print(f"
Processing query: '{query}'")

    # 1. Use the router agent to decide which specialist to use
    router_result = await Runner.run(router_agent, input=query)
    route_decision = router_result.final_output_as(RouteDecision)

    print(f"Router decision: Route to '{route_decision.specialist_name}'. Reasoning: {route_decision.reasoning}")

    # 2. Route to the chosen specialist agent
    chosen_specialist_name = route_decision.specialist_name
    if chosen_specialist_name in specialist_agents:
        specialist_agent_to_run = specialist_agents[chosen_specialist_name]
        print(f"Running {specialist_agent_to_run.name}...")
        specialist_result = await Runner.run(specialist_agent_to_run, input=query)
        final_answer = specialist_result.final_output_as(str)
        print(f"Specialist Answer: {final_answer}")
    else:
        print(f"Error: Unknown specialist '{chosen_specialist_name}' decided by router.")


async def main() -> None:
    await process_query("What is 2 + 2?")
    await process_query("Who was the first president of the United States?")
    # Example of a query that might be ambiguous for the simple router
    await process_query("Tell me about the history of mathematics.")


if __name__ == "__main__":
    asyncio.run(main())
```

### `agent_lifecycle_example.py`
This example highlights **Observability & Debugging** (Section 1.2) by demonstrating the use of `AgentHooks` for **Tracing & Observability** (Section 6.2) to monitor an `Agent`'s lifecycle events.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
from typing import Any

from agents import Agent, AgentHooks, ModelResponse, RunContextWrapper, Runner, TResponseInputItem, ToolCallOutputItem


class MyAgentHooks(AgentHooks[Any]):
    async def on_agent_start(
        self, *, agent: Agent, context: RunContextWrapper[Any], input: str | list[TResponseInputItem]
    ) -> None:
        print(f"Agent '{agent.name}' starting.")

    async def on_agent_finish(
        self,
        *,
        agent: Agent,
        context: RunContextWrapper[Any],
        input: str | list[TResponseInputItem],
        response: ModelResponse,
    ) -> None:
        print(f"Agent '{agent.name}' finished.")

    async def on_tool_call_start(
        self,
        *,
        agent: Agent,
        context: RunContextWrapper[Any],
        tool_name: str,
        tool_input: str,
    ) -> None:
        print(f"Tool '{tool_name}' starting with input: {tool_input}")

    async def on_tool_call_finish(
        self,
        *,
        agent: Agent,
        context: RunContextWrapper[Any],
        tool_call_output_item: ToolCallOutputItem,
    ) -> None:
        print(f"Tool '{tool_call_output_item.tool_name}' finished with output: {tool_call_output_item.output}")


# Create an agent with the custom hooks
agent_with_hooks = Agent(
    name="LifecycleDemoAgent",
    instructions="You are a helpful assistant.",
    hooks=MyAgentHooks(),
)


async def main() -> None:
    print("Running agent with lifecycle hooks:")
    result = await Runner.run(agent_with_hooks, "Hello!")
    print(f"Final output: {result.final_output}")


if __name__ == "__main__":
    asyncio.run(main())
```

### `customer_service/main.py`
This comprehensive example ties together multiple principles: **Modularity & Composability** (triage and specialist agents), **Hierarchical Planning** (triage routing to specialists using `handoffs`), **Contextual Tools** (using `RunContextWrapper`), and **Tool Design Patterns**. It's a good illustration of a more complex multi-agent system.
```python
# Copyright (C) 2024, OpenAI, Inc.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations as _annotations

import asyncio
import random
import uuid

from pydantic import BaseModel

from agents import (
    Agent,
    HandoffOutputItem,
    ItemHelpers,
    MessageOutputItem,
    RunContextWrapper,
    Runner,
    ToolCallItem,
    ToolCallOutputItem,
    TResponseInputItem,
    function_tool,
    handoff,
    trace,
)
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

# This is an example of a customer service system for an airline.
# It includes a triage agent that can hand off to specialist agents for specific tasks.
# It also includes tools for looking up FAQs and booking information.
# The specialist agents are configured with descriptions that help the triage agent
# decide which agent to hand off to.

### CONTEXT
# Define a context object that can be used by all agents and tools.
# This allows sharing information like passenger name, confirmation number, etc.
class AirlineAgentContext(BaseModel):
    passenger_name: str | None = None
    confirmation_number: str | None = None
    seat_number: str | None = None
    flight_number: str | None = None

### TOOLS
# Define tools that can be used by the agents.
# These tools simulate looking up FAQs and booking information.
@function_tool(
    name_override="faq_lookup_tool",
    description_override="Lookup frequently asked questions.",
)
async def faq_lookup_tool(question: str) -> str:
    """
    Lookup frequently asked questions.

    Args:
        question: The question to lookup.
    """
    print(f"Looking up FAQ for question: {question}")
    if "baggage allowance" in question.lower():
        return "For economy class, you are allowed one carry-on bag and one personal item. Checked bags are subject to fees."
    elif "check-in time" in question.lower():
        return "We recommend arriving at the airport 2 hours before domestic flights and 3 hours before international flights."
    elif "pet policy" in question.lower():
        return "Small pets are allowed in the cabin for a fee, provided they are in an approved carrier. Larger pets must be transported as cargo."
    return "I couldn't find an answer to that specific question in the FAQs. Please provide more details or ask something else."


@function_tool(
    name_override="booking_lookup_tool",
    description_override="Lookup booking information using confirmation number or passenger name.",
)
async def booking_lookup_tool(
    context: RunContextWrapper[AirlineAgentContext],
    confirmation_number: str | None = None,
    passenger_name: str | None = None,
) -> str:
    """
    Lookup booking information using confirmation number or passenger name.

    Args:
        confirmation_number: The booking confirmation number.
        passenger_name: The name of the passenger.
    """
    print(f"Looking up booking for confirmation: {confirmation_number}, passenger: {passenger_name}")
    # In a real system, this would query a database.
    # For this example, we'll use the context or return a mock response.
    if context.context.confirmation_number and confirmation_number == context.context.confirmation_number:
        return f"Booking found for {context.context.passenger_name or 'passenger'}. Flight {context.context.flight_number or 'UA123'} on 2024-07-15, Seat {context.context.seat_number or '12A'}."
    if passenger_name and context.context.passenger_name and passenger_name.lower() in context.context.passenger_name.lower():
        return f"Booking found for {context.context.passenger_name}. Flight {context.context.flight_number or 'UA123'} on 2024-07-15, Seat {context.context.seat_number or '12A'}."
    if confirmation_number == "ABC123":
        return "Booking found for John Doe. Flight UA456 on 2024-07-16, Seat 22B."
    return "I couldn't find a booking with that information. Please double-check the details."


### SPECIALIST AGENTS
# Define specialist agents that can handle specific tasks.
# These agents are given descriptions that help the triage agent decide when to hand off to them.

flight_change_agent = Agent[AirlineAgentContext](
    name="Flight Change Specialist",
    instructions="You are a flight change specialist. Assist the user with changing their flight. "
    "Confirm their booking details first using available tools if necessary.",
    tools=[booking_lookup_tool],
    handoff_description="This agent handles requests related to changing flight dates, times, or destinations.",
)

baggage_agent = Agent[AirlineAgentContext](
    name="Baggage Claims Agent",
    instructions="You are a baggage claims specialist. Assist the user with lost or damaged baggage.",
    tools=[booking_lookup_tool, faq_lookup_tool],
    handoff_description="This agent handles issues related to lost, delayed, or damaged baggage.",
)

refund_agent = Agent[AirlineAgentContext](
    name="Refund Specialist",
    instructions="You are a refund specialist. Assist the user with refund requests for flights or add-ons. "
    "Verify booking details and eligibility using available tools.",
    tools=[booking_lookup_tool, faq_lookup_tool],
    handoff_description="This agent processes refund requests for flights, seat upgrades, or other ancillary services.",
)

# A generalist agent that can answer FAQs or simple queries.
general_inquiry_agent = Agent[AirlineAgentContext](
    name="General Inquiry Agent",
    instructions="You are a general inquiry agent. Answer questions using the FAQ tool. "
    "If the question is complex or requires booking information, state you may need to handoff.",
    tools=[faq_lookup_tool],
    handoff_description="This agent can answer general questions based on FAQs.",
)


### TRIAGE AGENT
# Define a triage agent that decides which specialist agent to hand off to.
# It uses the handoff_description of the specialist agents to make this decision.
triage_agent = Agent[AirlineAgentContext](
    name="Airline Customer Service Triage",
    instructions=f"{RECOMMENDED_PROMPT_PREFIX}You are the first point of contact for airline customer service. "
    "Your primary role is to understand the user's query and hand off to the appropriate specialist agent if necessary. "
    "You can also use the FAQ tool for simple questions. "
    "If handing off, explain briefly why you are doing so. "
    "If the user asks multiple questions, address them one by one, potentially handing off for each if needed.",
    tools=[faq_lookup_tool],
    handoffs=[
        flight_change_agent,
        baggage_agent,
        refund_agent,
        handoff(
            general_inquiry_agent,
            # Example of a custom on_handoff hook
            on_handoff=lambda ctx, agent_input: print(
                f"Triage handing off to General Inquiry for input: {agent_input}"
            ),
        ),
    ],
)


async def run_conversation_turn(
    current_agent: Agent[AirlineAgentContext],
    user_input: str,
    context_obj: AirlineAgentContext,
    previous_items: list[TResponseInputItem] | None = None,
) -> tuple[Agent[AirlineAgentContext], list[TResponseInputItem]]:
    """Simulates a single turn in the conversation."""
    print(f"
--- User: {user_input} ---")
    current_turn_input: str | list[TResponseInputItem] = (
        (previous_items + [{"role": "user", "content": user_input}]) if previous_items else user_input
    )

    result = await Runner.run(
        starting_agent=current_agent,
        input=current_turn_input,
        context=context_obj,
        # Example of setting a max_turns for a specific run
        # max_turns=5
    )

    next_agent = result.last_agent
    all_new_items = result.new_items

    print(f"--- Agent: {current_agent.name} processed ---")
    for item in all_new_items:
        if isinstance(item, MessageOutputItem):
            print(f"  Message from {result.history_items_to_new_items_snapshot[item][0].name}:") # type: ignore
            print(f"    {ItemHelpers.text_message_output(item)}")
        elif isinstance(item, ToolCallItem):
            print(f"  Tool Call by {result.history_items_to_new_items_snapshot[item][0].name}: {item.tool_name}") # type: ignore
            print(f"    Input: {item.tool_input}")
        elif isinstance(item, ToolCallOutputItem):
            print(f"  Tool Output for {item.tool_name}:")
            print(f"    Output: {item.output}")
        elif isinstance(item, HandoffOutputItem):
            print(f"  Handoff from {item.source_agent.name} to {item.target_agent.name}")
            if item.tool_input:
                print(f"    Handoff reason/input: {item.tool_input}")
            next_agent = item.target_agent  # Update agent based on handoff

    if result.final_output: # Should always be true unless error
        print(f"  Final Output from {result.last_agent.name}: {result.final_output}")


    # Construct the full history for the next turn
    updated_history = list(result.input_if_string_converted_to_list) + all_new_items # type: ignore
    return next_agent, updated_history


async def main() -> None:
    # Initialize context for the conversation
    # In a real app, this might come from a user session or database
    conversation_id = str(uuid.uuid4())
    passenger_name = "Alice Wonderland"
    confirmation_number = f"XYZ{random.randint(100,999)}"
    flight_number = f"OA{random.randint(100,999)}"
    seat_number = f"{random.randint(1,30)}{random.choice(['A','B','C','D','E','F'])}"

    airline_context = AirlineAgentContext(
        passenger_name=passenger_name,
        confirmation_number=confirmation_number,
        flight_number=flight_number,
        seat_number=seat_number,
    )
    print(f"Simulating conversation: {conversation_id}")
    print(f"Passenger: {passenger_name}, Confirmation: {confirmation_number}, Flight: {flight_number}, Seat: {seat_number}")


    # Start with the triage agent
    current_agent_for_turn: Agent[AirlineAgentContext] = triage_agent
    conversation_history: list[TResponseInputItem] = []

    # Wrap the entire conversation in a trace
    with trace("Airline Customer Service Conversation", group_id=conversation_id):
        # Turn 1: General FAQ
        user_query = "What's your baggage allowance?"
        current_agent_for_turn, conversation_history = await run_conversation_turn(
            current_agent_for_turn, user_query, airline_context, conversation_history
        )

        # Turn 2: Query requiring booking lookup (should use tool)
        user_query = f"Can you tell me my seat number? My confirmation is {confirmation_number}."
        current_agent_for_turn, conversation_history = await run_conversation_turn(
            current_agent_for_turn, user_query, airline_context, conversation_history
        )
        # Note: Triage might handoff here if it can't answer directly or if its logic dictates.
        # The run_conversation_turn function updates current_agent_for_turn based on handoffs.

        # Turn 3: Query that should trigger a handoff to Flight Change Specialist
        user_query = "I need to change my flight to tomorrow."
        current_agent_for_turn, conversation_history = await run_conversation_turn(
            current_agent_for_turn, user_query, airline_context, conversation_history
        )

        # Turn 4: Follow-up to the specialist
        user_query = "Okay, please look for flights in the morning."
        # current_agent_for_turn is now likely FlightChangeSpecialist
        current_agent_for_turn, conversation_history = await run_conversation_turn(
            current_agent_for_turn, user_query, airline_context, conversation_history
        )

        # Turn 5: Query that should trigger handoff to Baggage Claims
        user_query = "My bag was damaged on my last flight."
        # We reset to triage for this new, distinct issue, or let current specialist attempt handoff
        # For this example, let's assume the user is starting a new topic with triage.
        # In a more complex system, the FlightChangeSpecialist might also be able to handoff.
        current_agent_for_turn = triage_agent
        # conversation_history = [] # Optionally reset history if starting "fresh" with triage for a new issue
        current_agent_for_turn, conversation_history = await run_conversation_turn(
            current_agent_for_turn, user_query, airline_context, conversation_history
        )


if __name__ == "__main__":
    # Uncomment to see very verbose logs from the SDK
    # import logging
    # logging.basicConfig()
    # logging.getLogger("openai.agents").setLevel(logging.DEBUG)
    # logging.getLogger("openai.agents.tracing").setLevel(logging.DEBUG)
    asyncio.run(main())
```

### Console Conversation Loop (Direct Input)
This example demonstrates building a basic interactive loop with an `Agent`, focusing on managing `conversation_history` (part of **Short-Term Context**, Section 3.1) and using `output_type` for structured agent decisions (like `should_terminate`).

```python
import asyncio
from pydantic import BaseModel, Field
from typing import List, cast

from agents import Agent, Runner, OpenAIChatCompletionsModel, TResponseInputItem
from config import get_external_client

# --- Output Schema for Responder Agent ---
class ResponderOutput(BaseModel):
    response_text: str = Field(description="The agent's textual response to the user.")
    should_terminate: bool = Field(description="Set to true if the conversation should end, otherwise false.")

# --- Client and Model Configuration ---
QWEN_MODEL_NAME = "qwen3-30b-a3b@q8_0"

client = get_external_client()
if client is None:
    raise ValueError("get_external_client() returned None. Check config.py and LM Studio server.")

# --- Agent Definitions ---

# Responder Agent: Decides what to say and if conversation should end.
responder_agent = Agent(
    name="ResponderAgentQwen",
    instructions=(
        "You are a conversational AI. Based on the conversation history (especially the user's last message), "
        "formulate a concise and relevant textual response. "
        "Also, analyze the context to decide if the conversation has naturally concluded or if the user wishes to end it. "
        "Your output MUST be a JSON object conforming to the ResponderOutput schema, with 'response_text' for your reply "
        "and 'should_terminate' (true/false)."
    ),
    model=OpenAIChatCompletionsModel(
        model=QWEN_MODEL_NAME,
        openai_client=client
    ),
    output_type=ResponderOutput
)

async def main():
    if not client:
        print("Cannot run agents because LM Studio client failed to initialize.")
        return

    print(f"Starting direct input conversational agent with model: {QWEN_MODEL_NAME}.")
    print("Conversation will continue until the ResponderAgent decides to terminate or you press Ctrl+C.")

    conversation_history: List[TResponseInputItem] = []
    turn_count = 0

    # Initial user message
    try:
        initial_user_message = await asyncio.to_thread(input, "You: ")
        if not initial_user_message.strip():
            print("No input received. Exiting.")
            return
    except KeyboardInterrupt:
        print("\nConversation ended by user (KeyboardInterrupt during initial input).")
        return
    
    conversation_history.append({"role": "user", "content": initial_user_message})

    while True:
        turn_count += 1
        print(f"\n--- Turn {turn_count} ---")

        # 1. Run ResponderAgent to get the agent's reply and termination decision
        print(f"ResponderAgent ({QWEN_MODEL_NAME}) is thinking...")
        responder_result = await Runner.run(
            starting_agent=responder_agent,
            input=list(conversation_history) 
        )

        if not responder_result or not responder_result.final_output:
            print("ResponderAgent failed to produce a response. Ending conversation.")
            break
        
        responder_output = responder_result.final_output_as(ResponderOutput)
        if not responder_output:
            print(f"ResponderAgent output could not be parsed as ResponderOutput. Raw: {responder_result.final_output}. Ending.")
            break

        print(f"Agent: {responder_output.response_text}")
        conversation_history.append({"role": "assistant", "content": responder_output.response_text})

        if responder_output.should_terminate:
            print("Agent has decided to end the conversation.")
            break

        # 2. Directly get next user input if conversation hasn't ended
        try:
            user_next_message = await asyncio.to_thread(input, "You: ")
            if not user_next_message.strip(): 
                print("Empty input received. Agent will process this or may terminate on next turn.")
                # Let the agent decide how to handle empty input
        except KeyboardInterrupt:
            print("\nConversation ended by user (KeyboardInterrupt).")
            break # Exit the while loop

        conversation_history.append({"role": "user", "content": user_next_message})
        # Debug prints from previous attempts removed for clarity now
    
    print("Conversation has ended.")

if __name__ == "__main__":
    asyncio.run(main()) 
```
